Nexora Cross-Functional Enterprise Review (2026)

Scope: Single authoritative, cross-functional review of the entire Nexora codebase (excluding *.md files), aligned strictly with Nexora's cybersecurity SaaS vision and MVP++++ requirements.

This document is populated from a line-by-line, file-by-file, folder-by-folder review by backend, frontend, security, infra, product/QA, and architecture perspectives. No other review documents are being created.

Status: Root architecture and configuration analysis started (frontend Next.js, backend Node/Express, ML service, Postgres, Redis, Vault, Prometheus/Grafana, Nginx, AWS/K8s).

=== 1. High-Level Architecture & Tech Stack ===
- Frontend: Next.js 14 (app router), React 18, TypeScript, TailwindCSS, Radix UI, Lucide icons.
- Backend API: Node.js 20, Express 4, Prisma ORM, PostgreSQL, Redis, Socket.IO, BullMQ, Vault integration, Kafka client present.
- ML Service: Python/FastAPI container (backend-ml) with Redis integration for models/scoring.
- Data Stores: PostgreSQL 16 (primary DB), Redis 7 (cache, queues, ML), Vault for secrets in dev, Prometheus + Grafana for metrics and dashboards.
- Infra/Deployment: Docker multi-stage images, docker-compose for local and production, optional Nginx reverse proxy with TLS, monitoring stack, K8s deployment manifest, AWS infra definitions under infrastructure/aws.

=== 2. Security & Threat Surface (Initial from Config-Level Review) ===
- Next.js security headers:
  - HSTS enabled only in production via next.config.js (correct pattern).
  - X-Frame-Options=DENY, X-Content-Type-Options=nosniff, Referrer-Policy=no-referrer, Permissions-Policy hardened for camera/mic/geo.
  - CSP: distinct dev vs prod policy; prod currently allows 'unsafe-inline' in script-src and wide connect-src to arbitrary HTTPS endpoints plus specific threat intel APIs.
- Middleware CSP integration:
  - middleware.ts injects a nonce and tries to replace "script-src 'self' 'strict-dynamic'" with a nonce-based directive, but the configured CSP in next.config.js does **not** contain that exact pattern (uses 'unsafe-inline' instead). This likely means the nonce injection never takes effect and CSP remains inline-script based. This is a concrete misalignment between security middleware and CSP configuration and should be treated as a bug.
- Network egress from frontend:
  - connect-src in CSP explicitly allows apiUrl, wsUrl, generic ws/wss/https, and specific external services (urlhaus, CISA, ip-api). This is consistent with a threat-intel heavy product but expands the browser egress surface. Needs to be enforced against a clear allowlist when finalizing production endpoints.

=== 3. Backend Implementation Review ===
- Entry and runtime:
  - backend Dockerfile: multi-stage build with non-root user, Prisma generate, healthcheck on /health, logs directory with least-privilege user. This is aligned with container hardening best practices.
  - backend/docker-compose.yml: composes Postgres, Redis, Vault, API, Prometheus, Grafana. Dev defaults embed weak example secrets (DB_PASSWORD, REDIS_PASSWORD, JWT_SECRET) via parameter expansion; acceptable for local/dev but must be **strictly overridden** in any shared or production-like environment.
  - API container is wired with DATABASE_URL using service DNS (postgres) and Redis via redis:// with password; healthcheck hits /health.
  - Vault is included in dev stack with root token passed via env; acceptable for dev only. No evidence yet of production auto-unseal/KMS wiring (to be checked under infrastructure/aws and backend src config).

- Env & configuration validation (env.ts):
  - Uses zod schema to validate process.env at startup and exits hard on misconfiguration, which is correct for a security platform.
  - Enforces presence of DATABASE_URL and REDIS_URL, and minimum length 32 for JWT_SECRET and JWT_REFRESH_SECRET, reducing risk of weak tokens.
  - ENCRYPTION_KEY and ENCRYPTION_IV are optional but have minimum length requirements when provided. Any crypto features that depend on them must be validated later to ensure they do not silently operate with missing keys.
  - ALLOWED_ORIGINS is a comma-separated string defaulting to http://localhost:3000, consumed by CORS. In production, this must be managed as a strict allowlist tied to Nexora tenants/deployments.

- Database and Redis clients (database.ts, redis.ts):
  - PrismaClient is configured with verbose query logging in development and error-only logging in production; this is an acceptable tradeoff between observability and log volume.
  - A separate pg Pool is created for raw OCSF threat events; connection pooling limits (max=20, idleTimeoutMillis=30000) are reasonable for an MVP but should be tuned under load.
  - Global singleton pattern (globalThis.__prisma, __pgPool) avoids client explosion in dev; beforeExit hook disconnects Prisma and ends pool, complementing the explicit shutdown in server.ts.
  - Redis is instantiated as four logical clients (general, queue, rateLimit, cache) all against env.REDIS_URL with a shared retry strategy capped at 3 attempts; enableReadyCheck is disabled, so readiness depends on external health checks rather than Redis’ internal view.
  - Redis connection events currently log to console instead of the structured logger; not a security flaw but a consistency gap for centralized observability.

- API security middleware (security.middleware.ts):
  - Helmet is configured with a CSP on the API side (default-src 'self', script-src 'self', style-src with 'unsafe-inline', no framing, strict referrer/permissions policy); for JSON APIs this is mostly defensive but consistent.
  - SQL injection protection middleware walks all strings in query/body and applies several regex patterns for SQL keywords and meta-characters. It blocks suspicious payloads with HTTP 400 and logs details. This is additive to Prisma parameterization and RLS, not a replacement.
  - XSS protection middleware strips script/iframe/javascript: patterns from all string values in body/query. This reduces risk when rendering any of these values later, but it can also mutate legitimate evidence content (e.g., raw logs, HTML samples). This tradeoff must be validated per route.
  - Request size limit enforces a hard cap based on Content-Length (default 10mb) and logs over-limit attempts; note it does not apply when Content-Length is missing, but body parsers also limit size.
  - Request ID and securityEventLogger tag each request with an ID and record 4xx/5xx responses as security events, integrating with metrics for rate-limit hits.

- Rate limiting (rateLimiter.middleware.ts):
  - Global rate limiter per IP uses RateLimiterRedis with window/durations driven by env.RATE_LIMIT_WINDOW_MS and RATE_LIMIT_MAX_REQUESTS; responses include standard rate limit headers and 429 status.
  - Organization rate limits are keyed by organizationId:ip, giving tenants higher aggregate limits than anonymous traffic and returning structured 429 with retry-after semantics.
  - Auth-specific limiter restricts login/refresh/forgot/reset to 5 attempts per 5 minutes with 15-minute blocks, which is aligned with industry practice for brute-force mitigation.
  - API key rate limiting is present and keyed by x-api-key, but its enforcement depends on upstream middleware actually authenticating API keys; this must be checked where API keys are consumed.

- RLS & multi-tenancy (rls.middleware.ts, tenant usage in routes):
  - enforceRowLevelSecurity sets a PostgreSQL session variable via set_current_organization(organizationId) using Prisma $executeRawUnsafe with bound parameters, then logs debug context. This relies on database-side RLS policies and the helper function implementation to be correct.
  - verifyRLSEnabled at server startup walks a fixed list of tables (identities, identity_activities, threats, baselines, observations, playbooks, actions, audit_logs, compliance_reports) and checks pg_tables.rowsecurity; any missing RLS is logged as an error and reflected in the health log at boot.
  - All tenant-sensitive feature routes (identities, threats, remediation, compliance, customer.*) mount requireAuth and tenantMiddleware at the router level before any handlers, which is consistent with multi-tenant enforcement in code.

- Subscription & feature gating (subscription.middleware.ts, auth.routes.ts):
  - checkSubscription enforces 7-day trial windows, past_due, and canceled states, returning 402 with explicit codes and upgrade URLs. It attaches structured subscription info and trialInfo to req for downstream use.
  - Tier limits (free/foundation/professional/enterprise) are encoded in TIER_LIMITS with explicit feature flags and limits on users/identities; requireFeature checks feature membership and returns 403 when unavailable.
  - User and identity limit checks use Prisma counts against org.maxUsers/maxIdentities; -1 is treated as unlimited. These checks are designed to be called on create flows.
  - Auth routes use authRateLimit and zod validators for register/login/refresh/forgot/reset, then requireAuth for profile/MFA/privileged operations. Admin user management endpoints are stubbed with requireRole(['admin']) but return 501 Not Implemented, which is explicit and safe at this stage.

- Audit logging & evidence chain (audit.middleware.ts, logger.ts):
  - auditMiddleware wraps res.send/json to capture response body metadata and, on finish, writes a structured record to prisma.auditLog and a parallel immutable evidence entry via evidenceService.writeEvidence().
  - It intentionally skips /health and /metrics to avoid noisy logs but logs all other routes, including method, path, IP, UA, duration, userId, and organizationId (via req.tenant where present).
  - sanitizeRequestBody redacts common sensitive fields (password, token, apiKey, mfaSecret, etc.) before persisting, reducing risk of secret leakage in logs.
  - logger.ts sanitizes log messages and metadata to avoid log injection (CWE-117) and sensitive data leakage, supports Loki transport when configured, and has dedicated [SECURITY] log method that tags events with securityEvent=true for SIEM correlation.

- Health and integrations monitoring (health.routes.ts):
  - /health aggregates checks for Postgres, Redis (redisRateLimit), Kafka, email, SIEM, and ticketing into a JSON response with overall status healthy/degraded and sets HTTP 200 vs 503 accordingly.
  - Fine-grained endpoints (/health/database, /health/redis, /health/kafka, /health/email, /health/siem, /health/ticketing, /health/kafka/detailed) expose component-specific health and configuration status (e.g., which SIEM/ticketing providers are configured).
  - These endpoints give operations teams good visibility but also expose internal topology and integration status if not properly shielded behind internal networks or authenticated paths.

- Authentication, sessions, MFA, and lockout (auth.middleware.ts, auth.controller.ts, token-rotation.service.ts, account-lockout.service.ts, auth.validator.ts, Prisma User/UserSession/RefreshToken/ApiKey):
  - Passwords are hashed with bcryptjs using 12 salt rounds on registration and change/reset flows, which is acceptable as of 2026 for web workloads (subject to periodic review under future hardware).
  - Input validation for auth endpoints uses strict zod schemas enforcing strong passwords (length, complexity) and 6-digit numeric MFA tokens; registration also constrains organizationName and fullName to sane character sets.
  - Access tokens (JWT) embed userId, organizationId, email, role and are signed with env.JWT_SECRET; refresh tokens are distinct and signed with env.JWT_REFRESH_SECRET, containing userId + sessionId + tokenVersion, which are bound to DB sessions for rotation.
  - `authenticate` middleware does more than JWT verification: it fetches the user (must be active) and requires at least one active, non-expired UserSession row, then updates lastActivity with a throttled write. This couples access tokens to server-side session state, which is a strong pattern for revocation.
  - Token rotation service (`TokenRotationService.refreshWithRotation`) validates the refresh token signature, loads the UserSession by sessionId, checks isActive + expiresAt, and enforces a tokenVersion match. On each refresh it increments tokenVersion and issues a new refresh token, invalidating replay attempts by bumping the version and deactivating the session if a stale token is seen.
  - Account lockout is implemented via `accountLockoutService` using Redis for counters and locks keyed by email/ip/combined; NIST-aligned defaults: 5 failed attempts in 5 minutes, 30-minute (progressively increasing) lockout, with Redis keys for `lockout:*` and `locked:*`. Auth controller login uses this service consistently (isLocked + recordFailedAttempt + clearFailedAttempts), and lockout events are written into AuditLog with severity bumped when a lock is triggered.
  - MFA uses TOTP via speakeasy; secrets are stored in `user.mfaSecret` (base32) and checked during login and verify/disable flows. Secrets are persisted in plaintext fields in the DB schema; there is no explicit application-layer encryption using ENCRYPTION_KEY/ENCRYPTION_IV, so DB-level protection and access controls are critical.
  - API keys are represented as `ApiKey` records with SHA-256 hashes of the key material (keyHash), organizationId, permissions string, and expiry; `requireApiKey` hashes the incoming x-api-key and builds a synthetic req.user with role "api". This is a solid baseline, but the permission model relies on downstream role/permission checks honoring the special role.
  - Request helper utilities normalize IP and user-agent extraction (`getClientIP`, `getUserAgent`) and centralize required parameter checks, which reduces inconsistency across controllers.

- Data model and multi-tenant boundaries (schema.prisma excerpt):
  - Core multi-tenant entities (`Organization`, `User`, `Identity`, `Threat`, `Incident`, `Action`, `Playbook`, `ComplianceReport`, `AuditLog`, `EvidenceLog`, `SecurityEvent`, NHITI, ThreatEvent, MalwareSample/MalwareIoc, HoneyToken, etc.) all carry either required or optional organizationId fields and are mapped to dedicated tables with explicit indexes on organizationId and time-based fields.
  - User model includes lockout and password reset fields (failedLoginAttempts, lockedUntil, lastFailedLogin, passwordResetToken, passwordResetExpires) aligned with the lockout and reset flows in the auth layer, enabling both in-memory (Redis) and persistent tracking.
  - EvidenceLog stores a full tamper-evident chain (prevHash, rowHash) plus organizationId, actor, resourceType/resourceId, payload, and retentionUntil, which matches the audit middleware’s design and supports defensible evidence retention for compliance.
  - Identity/Threat/Baseline/Observation schemas reflect Nexora’s non-human identity focus and real-time threat modeling: each ties back to Organization and Identity where appropriate, with JSON/text columns for metadata, baselineData, and observations to accommodate evolving schemas.
  - ThreatEvent and Nhiti* models are OCSF/STIX-aligned and include optional organizationId, enabling OSINT and NHITI data to be associated to tenants (or global) depending on ingestion logic.

- Identity & threat management flows (identities.controller/service, threats.controller/service, validators, repositories):
  - All identity and threat controllers derive organizationId strictly from req.tenant, which is established by tenantMiddleware after auth. Services and repositories consistently include organizationId in where clauses, aligning with the RLS design and reducing cross-tenant leakage risk.
  - Identity creation/enrichment:
    - createIdentitySchema strictly constrains type/provider/risk/status enums and enforces metadata size limits (10KB JSON), owner email validity, and filters disposable email domains for owner, which is appropriate for enterprise NHI tracking.
    - IdentityService encrypts credential values at creation via encryptionService.encrypt when `encrypted` is false and then forces `encrypted: true` before persisting JSON into the credentials column. This ensures credentials are not stored in cleartext when the service is used correctly.
  - Identity lifecycle and remediation:
    - list/search/stats are org-scoped and support pagination and filters on type/provider/status/risk/owner; stats aggregation currently loads all identities into memory and counts in-process (see gaps for DoS implications at scale).
    - rotateCredentials and quarantine operations verify existence via getById (scoped to org) before applying changes and recording activities. Quarantine elevates status to "quarantined" and riskLevel to "critical" and writes an identity_quarantined activity record.
    - Quarantine can optionally perform real network-level quarantine by calling awsQuarantineService.quarantineIP using IPs from identity.metadata (lastSeenIP/sourceIP) when the AWS service is configured. This is powerful but depends on metadata integrity and the AWS helper for final IP validation.
    - Identity activities and ML integration: recordActivity persists behavior events and, asynchronously, calls mlIntegrationService to analyze anomalies. For high-risk ML outcomes it auto-creates corresponding threats via threatService.create, which links behavioral anomalies to concrete threat records.
  - Threat modeling and investigation:
    - Threat validator schemas enforce well-defined severity/status/category values, constrain indicators (IP, hashes, domains) and evidence (logs, URLs, timestamps), and validate MITRE IDs (e.g., T1078, T1078.001).
    - ThreatService list/search APIs always filter on organizationId and limit per-page results to at most 100, with flexible filters over severity/status/category/identity/assignedTo and date windows.
    - Threat records store indicators and evidence as JSON strings; ThreatService/ThreatService.fixed validate and parse these fields with strict Zod schemas on both write and read, mitigating deserialization and resource exhaustion issues.
    - Status transitions (open → investigating → resolved/false_positive) and assignment are encapsulated in dedicated methods (updateStatus, investigate, remediate), and all perform an existence check via getById under the tenant scope.
    - Threat statistics endpoints aggregate by severity/status/category and expose counts plus recent/active threats for dashboards; the fixed service version moves these aggregations to the database via groupBy, while the older implementation still loads all threats in memory for some counts (see gaps).

- OSINT, threat intel, and NHITI sharing (osint.routes, intel.routes/service, threat-feed.routes, services/osint/*, services/threat-intel/*, nhiti.service.ts):
  - OSINT REST endpoints under /api/v1/osint provide latest/map/stats/blocklist views over ThreatEvent data and derived blocklists. They do **not** use requireAuth or tenantMiddleware, and instead accept an optional organizationId query parameter. This means OSINT data is effectively public at the API layer and can be filtered by arbitrary organizationId, which is inconsistent with the rest of the multi-tenant model and could leak tenant-specific threat intel if organizationId is used.
  - OSINT manual ingestion trigger (/api/v1/osint/ingest/trigger) is also unauthenticated. It instantiates an orchestrator and runs ingestion on demand, which is heavy and should not be internet-exposed; it should be restricted to internal operators or protected by strong auth/role checks.
  - Threat feed endpoints (/api/v1/threat-feed/...) are protected by requireAuth and use Prisma to query aggregated ingestion stats and global malware IOCs (organizationId='global'), which is consistent with a shared global feed model. However, they instantiate a new PrismaClient locally instead of using the shared DB config, which increases connection count and diverges from the central DB abstraction.
  - The high-level intel sharing service (intel.service.ts) uses an in-memory Map<intelId, ThreatIntel> as a process-local store and explicitly notes this is a placeholder for Redis/DB. It is **not** scoped by organizationId, so contributed IOCs are shared across tenants (by design) but are only keyed by type+value. There is no persistence, eviction, or rate limiting, so long-running processes will accumulate intelStore entries unbounded.
  - The NHITI service v2 (nhiti.service.ts) is significantly stronger: it hashes organization identifiers and IOCs with SHA-256, tracks k-anonymity (contributingOrg count), applies differential privacy parameters (epsilon, noiseApplied), enforces query rate limits (max queries/hour), logs queries with retention, and uses Prisma models (NhitiIndicator, NhitiParticipation, NhitiQueryLog) to persist sharing and participation metrics. This matches Nexora’s privacy-preserving non-human identity threat sharing vision and aligns with STIX/TAXII guidance.

- PQC and cryptography (pqc.routes.ts, services/pqc/* high-level view):
  - PQC routes expose public capability discovery and self-test endpoints, with all keygen/encapsulate/sign/verify operations gated behind requireAuth, which is the correct split between marketing/demo and sensitive crypto operations.
  - PQC service code (pqc.service.ts, high-level) integrates NIST-approved KEM/DSA/SLH-DSA algorithms and provides key management and operations via typed methods. The security of this layer will depend on correct algorithm selection, key storage, and integration with ENCRYPTION_KEY/ENCRYPTION_IV or KMS; current code suggests intent but must be re-verified before marketing as production-safe PQC.

- SOC operations, deception, and workflow remediation (soc.routes.ts, remediation.service.ts, workflow-remediation.service.ts, evidence.service.ts integration):
  - SOC routes are uniformly protected with requireAuth and, for sensitive actions (honey tokens, lineage/forensics, rollback, cloud isolation/quarantine, ticketing), requireRole(['admin', 'security_analyst']). This correctly restricts high-blast-radius operations to privileged roles.
  - Honey token creation and alert endpoints operate under tenant scope via organizationId on the token model; alerts carry severity and rich metadata (ip, UA, path, headers) and are stored in HoneyToken/HoneyTokenAlert tables, feeding SOC workflows.
  - RemediationService defines strongly typed playbook triggers and actions with Zod (rotate credentials, quarantine, block IP, revoke token, disable user, isolate instance, modify security groups, etc.), plus parameters such as cloudProvider, blastRadius, timeouts, retries, and requiresApproval. Execution path integrates with remediation executor and produces structured actionResults with explicit dry-run vs production modes.
  - WorkflowRemediationService layers a workflow engine on top of actions with step types (action/approval/condition/parallel/notification), approval gates, rollback via RemediationRollback records, and notifications via emailService + notificationQueueService. It also logs to Prisma for execution history and uses ticketing integrations where configured. This is consistent with enterprise SOAR playbooks.

- Explainable AI and ML accountability (explainable-ai.routes.ts, explainable-ai.service.ts):
  - Explainable AI routes requireAuth and expose endpoints for on-demand prediction explanations, per-identity explanation history, human review requests, and health checks.
  - The service coordinates with an external ML service via HTTP, stores explanations and local predictions in dedicated tables (MLPrediction, etc.), and writes audit logs for human review requests tagged for GDPR Article 22. Health checks call the ML service /health endpoint and reflect availability into a simple healthy/ml_service status.
  - Feature-level details, contributions, and counterfactuals are persisted; there is no obvious per-field masking at this layer, so upstream controllers must ensure that features logged do not contain personal data beyond what Nexora’s privacy model allows.

=== 4. Frontend Implementation Review ===
- Initial observation from config:
  - Dockerfile.frontend: multi-stage Next.js build, non-root user, healthcheck using /api/healthz, telemetry disabled, environment propagated via build args and runtime env.
  - Lighthouse CI config (JS/JSON) enforces aggressive performance, accessibility, and SEO thresholds. This is consistent with enterprise UX and reliability expectations.

- Root layout, consent, and analytics (app/layout.tsx, ConsentProvider, AnalyticsProvider, lib/analytics/index.ts):
  - Root layout wraps the entire app with ConsentProvider, AnalyticsProvider, a global Toaster, and a persistent ConsentBanner. This is positive from a privacy/compliance perspective: analytics are expected to be gated by explicit consent rather than always-on tracking, and the UX makes consent configurable across necessary/functional/analytics/marketing categories.
  - ConsentProvider persists consent in `localStorage` under `nexora_consent_v1` with timestamp and region (time zone) and, in production, sends a fire-and-forget POST to `/api/consent-log` with the full consent payload. This creates an auditable trail of consent changes but also means backend must treat consent logs as personal data under GDPR/CCPA and secure that endpoint appropriately.
  - AnalyticsProvider only calls `initAnalytics`/`initMarketing` when the corresponding consent flags are true. `initAnalytics` conditionally loads GA4 via gtag.js with `anonymize_ip: true` and secure cookie flags. This is an explicit, code-level enforcement of consent gating for third-party analytics, which is aligned with Nexora’s compliance positioning, but it further increases the importance of correcting the CSP/nonce story so that injected scripts remain tightly controlled.

- Public marketing/landing page (app/page.tsx):
  - Landing page is a client component that fetches platform stats from `/api/stats` every 30s with `cache: 'no-store'` and renders them as live counters. This is consistent with the product story but introduces an externally visible "live metrics" endpoint that must be validated for rate limiting and information disclosure (e.g., ensure stats are aggregate and non-sensitive, and protected behind CDN caching or server-side throttling).
  - The rest of the page focuses on static marketing content and UX components (hero, pillars, demo CTA, comparison matrix, pricing preview, FAQ). No obvious client-side security anti-patterns (no token handling, no direct backend calls beyond `/api/stats`). External links to /demo, /docs, /security, etc. are internal routes within the same Next app.

- Auth UX, signup, and login flows (app/auth/login/page.tsx, app/auth/signup/page.tsx):
  - Login form uses a Zod-based `loginFormSchema` for client-side validation of email/password and explicitly stores the password only via a ref, not in React state. This is a good practice to reduce the chance of accidentally logging/snapshotting passwords in dev tools.
  - The login page POSTs directly to `/api/v1/auth/login` and on success writes a bare `accessToken` into `localStorage` under the key `accessToken`, then navigates to `/admin` or `/client-dashboard` based on the returned `user.role`. This bypasses the shared token structure (`AuthTokens` with `expiresAt`/refreshToken) and does not integrate with the cookie-based `secureApiClient` at all.
  - The signup page POSTs to `/api/v1/auth/register` with organizationName/email/password/fullName/subscriptionTier and, on success, stores `accessToken` and `refreshToken` in `localStorage` under separate keys (`accessToken`, `refreshToken`) before routing to `/client-dashboard`. These tokens again are not wired into the central axios client (`nexora_tokens`) nor the `auth_token` expected by `src/lib/api/client.ts`.
  - Across the frontend there are **four different token storage patterns**: `accessToken`/`refreshToken` from signup, `accessToken` from login, `nexora_tokens` (structured tokens) in `src/services/api.ts`, and `auth_token` consumed by `APIClient` in `src/lib/api/client.ts`, plus cookie-based sessions expected by `secureApiClient`. This fragmentation will lead to inconsistent auth behavior, “half-logged-in” states, and is materially below enterprise-grade expectations for a security product.
  - Demo credentials (client@demo.com/admin@demo.com, password demo123) are hard-coded into the login UI. This is acceptable only for an isolated demo tenant and must never point at production auth. The presence of a known weak password in marketing UI should be carefully isolated at the routing and infrastructure layers.

- Frontend API clients and token handling (src/services/api.ts, src/lib/api/client.ts, src/lib/api/secure-client.ts, client/admin dashboards):
  - `src/services/api.ts` creates an axios instance with bearer-token auth from `nexora_tokens` in localStorage, internal rate limiting, SSRF checks, CSRF header, and a 401 handler that attempts refresh via `/api/auth/refresh` and redirects to `/login` on failure. Both the refresh path and redirect route conflict with the backend prefix (`/api/v1`) and actual login route (`/auth/login`), so this flow is broken as implemented.
  - `APIClient` in `src/lib/api/client.ts` is a thin fetch wrapper that reads `auth_token` from localStorage and attaches it as `Authorization: Bearer ...`. It backs the admin organizations API (`/admin/organizations...`) and is also used as a facade over `secureApiClient` for analytics/entities/threats. Since no code path sets `auth_token`, this client will only work if the backend also issues and validates cookies, or if some other code (not yet observed) populates `auth_token`.
  - `secureApiClient` is an axios-based enterprise-grade client that assumes **HTTP-only session cookies plus CSRF**: it sets `withCredentials: true`, fetches a CSRF token from `/auth/csrf`, injects `X-CSRF-Token` on state-changing requests, and redirects to `/auth/login` on 401. This matches the desired security posture but is effectively a parallel stack: the login/signup pages currently do not integrate with this client or mention cookie-based sessions.
  - Client dashboard layout (`app/client-dashboard/layout.tsx`) on logout only removes `localStorage['token']` and then routes to `/auth/login`. This neither clears `accessToken`/`refreshToken` nor `nexora_tokens` nor `auth_token`, so different parts of the app may still treat the user as authenticated.

- OSINT frontend hooks and dashboards (src/hooks/use-osint.ts, src/components/osint/*, client dashboard OSINT section):
  - OSINT hooks (`useOsintThreats`, `useOsintThreatMap`, `useOsintStats`, `useBlocklist`) all fetch directly from `/api/v1/osint/...` endpoints using the browser fetch API, with `cache: 'no-store'` and aggressive polling intervals (30s for threats/stats, 60s for map, 5 minutes for blocklist). No auth headers or tenant context are attached, which matches the backend design where these endpoints are unauthenticated and accept an optional `organizationId`.
  - `OsintMetrics`, `OsintThreatFeed`, and `BlocklistPanel` render these unauthenticated OSINT responses into the client dashboard and allow one-click copy/download of blocklists into plain-text files. This is powerful for SOC operations but also means that **anyone** with browser access to the dashboard host can, in principle, call these OSINT endpoints directly and exfiltrate the same data unless network-level controls are added. It amplifies the backend OSINT exposure risk already identified.
  - The heavy polling intervals and lack of client-side backoff mean a large number of tenants/users with dashboards open could generate sustained load against the `/api/v1/osint/*` endpoints. Combined with backend in-memory intel structures, this could be a scalability and DoS concern without further rate limiting and caching.

- Client dashboard surfaces (app/client-dashboard/*) and SOC tooling flows:
  - Core client dashboard pages for entities, threats, compliance, and reports (`entities/page.tsx`, `threats/page.tsx`, `compliance/page.tsx`, `reports/page.tsx`) are currently backed by **static/demo data only** and do not call the secured backend APIs (`/api/v1/identities`, `/api/v1/threats`, `/api/v1/compliance/reports`, etc.). This is safe for demos but means there is a gap between the rich backend feature set and what the product UI actually exercises.
  - The forensics page (`client-dashboard/forensics/page.tsx`) issues a direct POST to `/api/v1/soc/timeline` with filters, then maps `timeline.events` into a normalized `TimelineEvent[]`. It uses bare `fetch` with JSON but no explicit Authorization header; it relies entirely on same-origin cookies for auth if present. Given the fragmented token model (cookies vs localStorage tokens), there is a risk that this critical forensic feature will behave inconsistently depending on how the user authenticated.
  - The lineage page (`client-dashboard/lineage/page.tsx`) calls three SOC endpoints: `/api/v1/soc/lineage/:identityId`, `/api/v1/soc/lineage/:identityId/ancestors`, and `/api/v1/soc/drift/:identityId`, again via raw `fetch` and without explicit Authorization headers. It expects rich graph, ancestry, and drift-analysis payloads, but there is no guarding in the UI against partial failures across these three endpoints.
  - The honey-tokens management page (`client-dashboard/honey-tokens/page.tsx`) uses `fetch` against `/api/v1/soc/honey-tokens` for list/stats and against `/api/v1/soc/honey-tokens/:id/*` for create/disable/rotate actions. This correctly targets the SOC honey-token backend but, like the other SOC flows, assumes that cookie-based auth exists. There is no integration with the structured audit/evidence UI to surface when honey-token alerts are written to evidence logs.
  - The SOC integrations page (`client-dashboard/integrations/page.tsx`) calls `/api/v1/soc/cloud/status` for Kubernetes/Azure/GCP integration status and `/api/v1/soc/tickets` to send a "test incident" through ServiceNow/Jira/Slack. These calls are again made via bare `fetch` without Authorization headers, relying on whatever cookie/session infrastructure exists. This matches the SOC routes on the backend (which use `requireAuth` + `requireRole`), but the frontend auth handling is not yet reliably wired to that model.
  - The ML anomalies page (`client-dashboard/ml/page.tsx`) reads from `/api/v1/customer/analytics/ml-anomalies`. This endpoint is per-tenant and high-sensitivity (ML risk scores, contributing factors). The page calls it with `fetch` and no Authorization header, relying on same-origin cookies; meanwhile the customer dashboard itself (`app/customer-dashboard/page.tsx`) uses a separate `fetchThreats` client that talks directly to `NEXT_PUBLIC_API_URL` with no cookie configuration. This split means customer views for threats vs ML anomalies are using two inconsistent HTTP stacks.
- Frontend threat-intel proxy and external data dependencies (app/api/threat-intel/route.ts, client dashboard live data):
  - The `/api/threat-intel` Next.js route is a **server-side proxy** that calls NIST NVD (`https://services.nvd.nist.gov/rest/json/cves/2.0`) and derives demo threats, metrics, and entity breakdowns. It explicitly states "NO FALLBACK, API MUST WORK" and throws when NVD returns no data, meaning the client dashboard’s live section will hard-fail if NVD is rate-limited or temporarily unavailable.
  - This proxy uses a fixed `User-Agent` header and no extra authentication; it does not implement circuit-breaking, backoff, or caching beyond `revalidate: 0`. For an MVP demo this is acceptable, but for production use it would need explicit timeouts, error budgeting, and a fallback data-source strategy.
  - The client dashboard main page (`client-dashboard/page.tsx`) fetches from `/api/threat-intel` every 5 minutes on the client side to refresh demo metrics and threat cards. Combined with the OSINT polling, this adds another periodic network dependency that must be understood and controlled for both reliability and privacy (NVD access patterns tied to tenant behavior).

- Admin dashboard and API clients (app/admin/*, components/admin/*, lib/api/system.ts, billing.ts, nhiti.ts):
  - The admin dashboard (`app/admin/page.tsx`) is a feature-rich view for multi-tenant platform management, composing `SystemHealth`, `BillingDashboard`, `NHITIFeed`, and `OrganizationDetail` components.
  - These components and their backing API clients (`fetchSystemHealth`, `fetchBillingDashboard`, `fetchNHITIFeed`, `fetchOrganizations`) all use raw `fetch` against the `/api/v1/admin/*` and `/api/v1/nhiti/*` backend endpoints. They do **not** include any explicit `Authorization` headers and therefore rely entirely on the browser sending same-origin session cookies for authentication and role enforcement (`requireRole(['admin'])` on the backend). This confirms that the admin panel is designed for the `secureApiClient` cookie/CSRF model, but the login/signup pages do not currently establish this type of session.
  - The NHITI feed client (`lib/api/nhiti.ts`) explicitly comments `// Add authentication headers if needed`, confirming that it is currently unauthenticated from the client's perspective, which matches the backend route configuration for the feed itself.

- Customer-facing views (components/customer/*, lib/api/threats.ts, identities.ts):
  - `ThreatsView.tsx` and `IdentitiesView.tsx` are live-data components that use `fetchThreats` and `fetchIdentities` respectively. These API clients target the correct customer-facing backend endpoints (`/api/v1/customer/threats` and `/api/v1/customer/identities`).
  - However, like the admin clients, they use raw `fetch` with no `Authorization` header, relying on a cookie-based session that the current login/signup flows do not create. This means these views will fail with 401 Unauthorized errors in the current end-to-end flow until the frontend auth model is unified around the `secureApiClient` pattern.

=== 5. Data, Storage & Database Review ===
- Docker-level:
  - Postgres 16-alpine with explicit encoding and collation, persistent volume, healthcheck via pg_isready.
  - Redis 7-alpine with appendonly enabled and maxmemory+LRU policy; password required in production compose file.
- Prisma schema and migrations under backend/prisma not yet fully reviewed; will be analyzed next for multi-tenant boundaries, RBAC, and auditability.

=== 6. Infrastructure, Deployment, Cloud, and Networking Review ===
- docker-compose.production.yml:
  - Full stack: postgres, redis, nexora-api, nexora-ml, nexora-frontend, optional nginx (TLS termination), Prometheus, Grafana.
  - Uses internal vs external bridge networks to segment API/DB from public exposure.
  - Enforces required environment variables for critical secrets in production (e.g., POSTGRES_PASSWORD, JWT_SECRET, JWT_REFRESH_SECRET, Grafana admin password) using Docker parameter expansion with `:?` guards.
- K8s deployment and infrastructure/aws to be reviewed in detail for namespace isolation, NetworkPolicies, IAM, and secret management.

- Kubernetes Deployment (k8s/deployment.yaml):
  - The deployment.yaml for the nexora-landing service demonstrates a production-grade, security-hardened configuration. Key strengths include:
    - Pod/Container Security Context: Enforces non-root execution (runAsUser: 10001), read-only root filesystem, disabled privilege escalation, dropped Linux capabilities, and a default seccomp profile. This significantly reduces the container's attack surface.
    - Resilience: Implements a RollingUpdate strategy, HorizontalPodAutoscaler (HPA) for CPU/memory, and a PodDisruptionBudget (PDB) to ensure high availability during voluntary disruptions.
    - Configuration: Uses a ConfigMap to inject the backend API URL, correctly separating configuration from the container image.

- AWS Infrastructure (infrastructure/aws/terraform/*):
  - VPC & Networking: The Terraform configuration defines a best-practice VPC using the terraform-aws-modules/vpc module, with multiple AZs, public/private subnets, and NAT Gateways. VPC Flow Logs are enabled in production. Crucially, it creates VPC endpoints for S3, ECR, Secrets Manager, and CloudWatch Logs, ensuring traffic to these AWS services stays within the AWS network for improved security and reduced cost.
  - Security Groups: Network segmentation is strong, with dedicated security groups for the ALB, ECS tasks, RDS, and ElastiCache. Ingress rules follow the principle of least privilege (e.g., RDS only accepts traffic from the ECS task SG on the Postgres port). The only identified gap is the overly permissive egress rule (0.0.0.0/0) on the ECS task security group; for a high-security application, this should be restricted to known destinations.
  - IAM: Follows least privilege by defining separate, tightly scoped IAM roles for ECS task execution (pulling images, getting secrets) and the tasks themselves (accessing secrets, writing logs, sending traces). The use of a static IAM user for GitHub Actions is functional but less secure than using OIDC, as it requires managing long-lived API keys.
  - Compute (ECS Fargate): The platform runs on ECS Fargate, which is a modern, serverless approach. Task definitions are secure, with logs encrypted and streamed to CloudWatch, and ECS Exec is enabled with KMS encryption for audited debugging access.
  - Data Services (RDS & ElastiCache): Both PostgreSQL and Redis are configured for high security and availability. They are deployed in private subnets, enforce encryption at-rest (with CMK) and in-transit, require strong authentication tokens (managed via Secrets Manager), and have multi-AZ failover enabled.
  - Secrets Management: Excellent. Strong, randomly generated passwords and secrets are created by Terraform and stored securely in AWS Secrets Manager. ECS tasks retrieve these secrets at runtime via their IAM role, completely avoiding hard-coded credentials.
  - Edge Security (ALB & WAF): The architecture is fronted by an Application Load Balancer that terminates TLS with a modern security policy and redirects all HTTP traffic to HTTPS. The ALB is protected by AWS WAF, which is configured with rate limiting and multiple AWS Managed Rule Sets (Common, Bad Inputs, SQLi, IP Reputation), providing a robust defense against common web attacks.

=== 7. Gaps, Risks, and Required Changes for MVP++++ Enterprise Readiness ===
- Concrete issues observed so far:
  - CSP/middleware mismatch: nonce-based CSP hardening is implemented in middleware.ts but the actual CSP header string in next.config.js does not include the pattern the middleware expects, so nonce replacement likely never occurs. This leaves 'unsafe-inline' in prod script-src, which is below enterprise expectations for a security product.
  - Wide frontend connect-src in CSP: necessary for threat-intel APIs but currently broad enough to allow arbitrary HTTPS/WebSocket egress. Requires a documented, enforced allowlist tied to Nexora’s actual integrations.
  - Dev/default secrets in backend/docker-compose.yml: acceptable for isolated local use but must be blocked from any shared/dev/test/prod clusters; needs a guardrail policy in infra and CI.
  - Backend SQLi/XSS sanitization middleware (security.middleware.ts) aggressively strips patterns from any string in body/query. This reduces injection risk but can modify legitimate structured payloads (for example, raw HTML or JavaScript evidence). This behavior must be validated route-by-route to ensure it does not undermine Nexora’s forensic accuracy.
  - Audit middleware relies on req.tenant.organizationId for organization attribution. Tenant middleware is applied on tenant-scoped routers, but any route that does not set tenant context will log with an empty organizationId, reducing traceability. There needs to be an explicit policy for which routes are tenant-bound vs global and how they are represented in audit/evidence.
  - Health endpoints under /health expose detailed dependency and integration status (DB, Redis, Kafka, email, SIEM, ticketing). These must not be internet-exposed without protection; they should be restricted to internal networks or guarded by IP allowlists/auth in the ingress layer.
  - API key rate limiting exists but depends on a separate API key authentication/authorization layer. Until that layer is fully verified, there is a risk of inconsistent enforcement for key-based integrations.
  - OSINT routes (/api/v1/osint/...) are unauthenticated and accept organizationId as a query parameter. This contrasts with the rest of the multi-tenant model and can permit unauthenticated access to tenant-filtered OSINT data and heavy blocklist generation, as well as an unauthenticated ingestion trigger. For MVP production, these endpoints must be restricted (authN+authZ and/or internal-only exposure) and their per-tenant semantics clearly defined.
  - Threat intel sharing via intel.service.ts currently uses an in-memory Map shared across all organizations without persistence, eviction, or rate limiting. This is acceptable as a temporary MVP mechanism but is not horizontally scalable and does not provide enterprise-grade durability or tenant controls.
  - Some statistics helpers (identity and threat stats in older services) still load all rows into memory to compute counts by field. On large datasets this could become a DoS vector and should be replaced with database-side aggregation everywhere (Prisma groupBy or equivalent).
  - Evidence routes (/api/v1/evidence) rely on upstream JWT security annotations but do not themselves attach requireAuth/tenantMiddleware. Given the sensitivity of the evidence log, these endpoints must be verified to be behind strong authN/authZ and tenant scoping at the Express router or ingress layer.
  - Frontend token/auth fragmentation: There are four parallel token storage patterns in the frontend (accessToken/refreshToken from signup, accessToken from login, nexora_tokens in services/api.ts, auth_token in lib/api/client.ts) plus a cookie-based secureApiClient. This will cause inconsistent auth behavior and is below enterprise expectations. The login/signup flows do not establish the cookie-based session expected by admin/customer dashboards, leading to 401s in production.
  - Client dashboard SOC/tooling pages (forensics, lineage, honey-tokens, integrations, ML anomalies) use bare fetch without explicit Authorization headers, relying on cookies. Given the fragmented auth model, these critical features will behave inconsistently until the frontend auth is unified around the secureApiClient pattern.
  - OSINT frontend hooks poll unauthenticated /api/v1/osint/* endpoints every 30–60 seconds. Combined with backend in-memory structures, this could become a scalability and DoS concern without client-side backoff and stricter rate limiting.
  - The /api/threat-intel Next.js proxy calls NIST NVD with no fallback or circuit-breaking; if NVD is rate-limited or down, the client dashboard live section will hard-fail. For production, this needs timeouts, error budgeting, and fallback data sources.
  - Infrastructure: The ECS task security group allows egress to 0.0.0.0/0. For a high-security application, this should be restricted to known destinations (e.g., VPC endpoints, specific external APIs). The GitHub Actions deployment uses a static IAM user with long-lived keys; migrating to OIDC would improve security.
  - Infrastructure: VPC Flow Logs are enabled only in production; for full observability and threat detection, consider enabling them in staging as well.

Further sections will be expanded with line-by-line backend (auth, identities, threats, OSINT, PQC, compliance), frontend, data, and infra findings as the review proceeds.